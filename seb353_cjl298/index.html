<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>12-DOF Quadruped Robot - ECE 4760 Final Project</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            font-size: 14px;
        }
        
        .container {
            display: flex;
            min-height: 100vh;
        }
        
        /* Sidebar Navigation */
        .sidebar {
            width: 180px;
            background-color: #2c3e50;
            color: white;
            padding: 15px;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            font-size: 13px;
        }
        
        .sidebar h2 {
            margin-bottom: 15px;
            font-size: 16px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 8px;
        }
        
        .sidebar nav ul {
            list-style: none;
        }
        
        .sidebar nav ul li {
            margin-bottom: 8px;
        }
        
        .sidebar nav ul li a {
            color: white;
            text-decoration: none;
            display: block;
            padding: 6px 8px;
            border-radius: 4px;
            transition: background-color 0.3s;
            font-size: 13px;
        }
        
        .sidebar nav ul li a:hover {
            background-color: #34495e;
        }
        
        /* Main Content */
        .main-content {
            margin-left: 180px;
            padding: 30px;
            width: calc(100% - 180px);
        }
        
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 28px;
        }
        
        h2 {
            color: #2c3e50;
            margin-top: 25px;
            margin-bottom: 12px;
            font-size: 20px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }
        
        h3 {
            color: #34495e;
            margin-top: 18px;
            margin-bottom: 10px;
            font-size: 17px;
        }
        
        h4 {
            color: #34495e;
            margin-top: 15px;
            margin-bottom: 8px;
            font-size: 15px;
        }
        
        .subtitle {
            color: #7f8c8d;
            font-size: 15px;
            margin-bottom: 25px;
        }
        
        p {
            margin-bottom: 12px;
        }
        
        ul {
            margin-left: 25px;
            margin-bottom: 12px;
        }
        
        ul li {
            margin-bottom: 6px;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 15px 0;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        
        .soundbite {
            background-color: #ecf0f1;
            padding: 15px;
            border-left: 4px solid #3498db;
            margin: 15px 0;
            font-style: italic;
            font-size: 14px;
        }
        
        pre {
            background: #f8f8f8;
            padding: 12px;
            border-radius: 4px;
            font-size: 12px;
            overflow-x: auto;
            border: 1px solid #ddd;
            line-height: 1.4;
        }
        
        code {
            background: #f0f0f0;
            padding: 2px 5px;
            border-radius: 3px;
            font-size: 12px;
            font-family: 'Courier New', monospace;
        }
        
        .file-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin: 15px 0;
        }
        
        .file-box {
            background-color: #f5f5f5;
            padding: 12px;
            border-left: 4px solid #3498db;
        }
        
        .file-box h4 {
            margin-top: 0;
            font-size: 14px;
        }
        
        .file-box pre {
            background: white;
            margin-top: 8px;
        }
        
        .control-flow {
            background: #f8f8f8;
            padding: 15px;
            border-radius: 4px;
            border: 1px solid #ddd;
            margin: 15px 0;
        }
        
        .control-flow pre {
            background: white;
            margin: 10px 0 0 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 13px;
        }
        
        table th, table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        
        table th {
            background-color: #3498db;
            color: white;
        }
        
        table tr:nth-child(even) {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Sidebar Navigation -->
        <aside class="sidebar">
            <h2>Navigation</h2>
            <nav>
                <ul>
                    <li><a href="#intro">Introduction</a></li>
                    <li><a href="#high-level">High Level Design</a></li>
                    <li><a href="#program-hardware">Program/Hardware Design</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#conclusions">Conclusions</a></li>
                    <li><a href="#appendix">Appendix</a></li>
                </ul>
            </nav>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <section id="intro">
                <h1>12-DOF Quadruped Robot with Environmental Mapping</h1>
                <p class="subtitle"><strong>Sarah Grace Brown (seb353) & Connor Lynaugh (cjl298)</strong><br>
                Cornell University - ECE 4760: Designing with Microcontrollers - Fall 2024</p>

                <div class="soundbite">
                    <strong>Project Sound Bite:</strong> A 12-DOF quadruped robot with 3 servos per leg and WiFi control that uses environmental feedback from an IMU, LiDAR, and infrared temp sensor to navigate, identify targets, and map its environment autonomously using distance and temperature data.
                </div>

                <h2>Summary</h2>
                <p>We designed and built a 12 degree-of-freedom (3 servos per leg × 4 legs) quadruped robot controlled by a Raspberry Pi Pico W, featuring integrated environmental sensing and a wireless WiFi controller. Starting from a custom CAD body and 3D-printed frame, the robot combines mechanical engineering and embedded electrical engineering to create a platform capable of coordinated four-legged locomotion, heading determination, environmental mapping, and target detection. In order to do so, our system leverages several sensors including an IMU, a solid state LiDAR sensor, and a contact-less infrared sensor.</p>

                <img src="images/robot_assembled.jpg" alt="Assembled Quadruped Robot">
                <p style="text-align: center; color: #7f8c8d; font-size: 13px;"><em>Figure 1: Assembled quadruped robot with sensor array</em></p>

                <h2>What We Did</h2>
                <p>Our project implements a fully functional walking robot with the following key features:</p>

                <h3>Mechanical Design</h3>
                <p>We created a custom 3D-printed body and leg assemblies optimized for our specific servo operation requirements and sensor placement. After encountering flexibility issues with initial prints due to insufficient infill, we redesigned the frame to provide rigid mounting points for all four legs and the twelve MG90S servos while accommodating the electronics and wiring on the body through implementing a 2-layer mounting system, enclosing all of the servo cables. The design includes TPU grippy feet for improved traction and stability. The leg files are based off an <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">existing online design</a>, but the attachment to the body and the body itself is all custom to allow for mounting of the Raspberry Pi and sensors.</p>

                <img src="images/cad_body_top.png" alt="CAD Body Design - Top View">
                <p style="text-align: center; color: #7f8c8d; font-size: 13px;"><em>Figure 2: Custom body CAD design - top view</em></p>

                <img src="images/cad_body_iso.png" alt="CAD Body Design - Isometric View">
                <p style="text-align: center; color: #7f8c8d; font-size: 13px;"><em>Figure 3: Custom body CAD design - isometric view showing 2-layer structure</em></p>

                <img src="images/cad_assembly_top.png" alt="Full Assembly - Top View">
                <p style="text-align: center; color: #7f8c8d; font-size: 13px;"><em>Figure 4: Complete robot assembly - top view</em></p>

                <img src="images/cad_assembly_iso.png" alt="Full Assembly - Isometric View">
                <p style="text-align: center; color: #7f8c8d; font-size: 13px;"><em>Figure 5: Complete robot assembly - isometric view</em></p>

                <h3>Locomotion System</h3>
                <p>Twelve servos provide three degrees of freedom per leg (joint rotation, thigh angle, and calf angle), controlled via an Adafruit PCA9685 16-channel PWM driver over I2C. We implemented coordinated gait patterns that enable the robot to walk forward, backward, turn in place, and complete fun gestures such as waving, squatting, and shuffling. We took initial servo positions and control sequences for these movements from a <a href="https://github.com/ConnorLynaugh/Digital-Design-with-Micro-Controllers" target="_blank">Python library</a> and adapted them for C.</p>

                <h3>Sensor Integration</h3>
                <p>The robot features numerous sensors that work together to allow for autonomous navigation and environmental mapping:</p>
                <ul>
                    <li><strong>MPU6050 IMU</strong> for heading determination and orientation tracking to map the environment</li>
                    <li><strong>TF-Luna LiDAR</strong> for distance ranging (0.2-8m) to detect obstacles and map surroundings</li>
                    <li><strong>MLX90614 infrared temperature sensor</strong> for heat signature detection and thermal mapping (effective range ~30cm for human-sized targets)</li>
                </ul>

                <img src="images/wiring_diagram.png" alt="System Wiring Diagram">
                <p style="text-align: center; color: #7f8c8d; font-size: 13px;"><em>Figure 6: Complete system wiring diagram</em></p>

                <h3>Wireless Control</h3>
                <p>A WiFi-based control interface allows real-time command and control from a computer or mobile device on the local network (we used an iPad), enabling remote operation and monitoring of the robot's sensor data.</p>

                <h2>Why We Built This</h2>
                <p>We wanted to create a robot that truly interacts with the world around it. Rather than building a machine that simply responds to pre-programmed commands, we set out to design a sensing platform that could perceive, understand, and navigate complex environments autonomously while moving in a unique and challenging way. The platform we built has the potential to execute various autonomous missions with different sensor-driven objectives.</p>

                <p>Inspired by how living creatures use multiple sensory inputs simultaneously, we aimed to replicate this approach of combining data types to make decisions in a robotic platform, and have it sense in ways humans can't—in this case, thermal detection. By combining LiDAR for spatial mapping, infrared sensing for thermal detection, and IMU data for orientation tracking, our quadruped builds an understanding of its surroundings and makes intelligent decisions about movement and target identification.</p>

                <p>This project represents the intersection of our mechanical and electrical engineering backgrounds. Coordinating twelve servos in real-time while processing sensor data and maintaining wireless communication pushed us to deeply understand the integration between hardware design, software architecture, and control systems. We chose a quadruped specifically because legged locomotion is more complex than wheeled robots and presents unique challenges in stability, gait coordination, and terrain adaptability.</p>

                <p>The practical applications are compelling: quadruped robots with environmental sensing have real-world potential in search and rescue operations (thermal sensing to locate people), industrial inspection in hazardous environments, and exploration of inaccessible terrain. Or even simpler, it can act as a pet and follow you around! Building from scratch gave us the flexibility to customize the sensor suite and control functions for these use cases while demonstrating that sophisticated autonomous robotics is achievable on an affordable microcontroller platform.</p>
            </section>

            <section id="high-level">
                <h2>High Level Design</h2>

                <h3>Rationale and Sources of Project Idea</h3>
                <p>Our project was inspired by existing quadruped robot platforms and open-source designs available online. We began by researching quadruped locomotion and found several reference designs on platforms like Instructables and GitHub that demonstrated the feasibility of servo-based legged robots.</p>

                <p><strong>Key inspirations included:</strong></p>
                <ul>
                    <li><strong>Mechanical Design Foundation:</strong> We adapted leg geometry from an <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">open-source 3D-printed quadruped design</a> that provided a proven kinematic structure for three-DOF legs. However, we completely redesigned the body and mounting system to accommodate our specific sensor suite and electronics layout.</li>
                    <li><strong>Locomotion Control:</strong> Initial servo position sequences and gait patterns were adapted from a Python-based <a href="https://github.com/ConnorLynaugh/Digital-Design-with-Micro-Controllers" target="_blank">quadruped control library</a>. We translated these movements into C and modified them for our specific servo channel mapping and timing requirements on the Pico W.</li>
                    <li><strong>Sensor Integration Approach:</strong> The decision to combine multiple sensor modalities (LiDAR, IR temperature, IMU) was driven by our goal to create a robot with perception capabilities beyond human senses. This multi-sensory approach mirrors research in autonomous mobile robotics where sensor fusion improves environmental understanding.</li>
                    <li><strong>Platform Selection:</strong> We chose the Raspberry Pi Pico W for its balance of computational capability, built-in WiFi, and I2C support, all at a low cost point that makes the project accessible.</li>
                </ul>

                <p>The core innovation in our project is not the individual components, but rather the integration of environmental mapping capabilities with legged locomotion on a resource-constrained embedded platform, creating an affordable autonomous sensing robot.</p>

                <h3>Background Math</h3>

                <h4>Servo PWM Control</h4>
                <p>The MG90S servos are controlled via PWM signals from the PCA9685 controller. Standard hobby servo control operates at 50 Hz (20ms period). The pulse width determines the servo angle:</p>
                <ul>
                    <li>1ms pulse → 0°</li>
                    <li>1.5ms pulse → 90° (neutral)</li>
                    <li>2ms pulse → 180°</li>
                </ul>

                <p>The PCA9685 uses 12-bit resolution (4096 steps). To calculate pulse length in counts:</p>
                <p style="text-align: center; font-style: italic;">pulse_length = (desired_ms / 20ms) × 4096</p>
                <p>For example, 90° with a 1.5ms pulse: (1.5 / 20) × 4096 = 307 counts</p>

                <p>Our servo control function maps angles to PWM values:</p>
                <pre>uint16_t angle_to_pwm(float angle) {
    // Map 0-180° to approximately 150-600 counts (calibrated for MG90S)
    return (uint16_t)(SERVOMIN + (angle / 180.0) * (SERVOMAX - SERVOMIN));
}

void set_servo_angle(uint8_t channel, float angle) {
    uint16_t pwm = angle_to_pwm(angle);
    pca9685_set_pwm(channel, 0, pwm);
}</pre>

                <h4>IMU Heading Calculation</h4>
                <p>The MPU6050 provides gyroscope data that we use for heading determination. We integrate angular velocity over time to track orientation:</p>
                <p style="text-align: center; font-style: italic;">heading(t) = heading(t-1) + ω<sub>z</sub> × Δt</p>
                <p>Where ω<sub>z</sub> is angular velocity around the z-axis (yaw rate) measured in degrees per second, and Δt is the time interval.</p>

                <pre>static inline void update_heading_from_gyro(void) {
    uint64_t now = time_us_64();
    float dt = (now - g_last_gyro_time) / 1000000.0f;
    if (dt < 0.002f) return;

    mpu6050_read_raw(acceleration, gyro);
    float gz = fix2float15(gyro[2]);
    gz += g_gyro_bias; // Apply bias compensation

    // Deadband filter to eliminate drift from small noise
    if (fabsf(gz) < 0.2f) gz = 0.0f;

    g_gyro_heading += gz * dt;

    // Keep heading in 0-360° range
    while (g_gyro_heading < 0) g_gyro_heading += 360.0f;
    while (g_gyro_heading >= 360.0f) g_gyro_heading -= 360.0f;

    g_last_gyro_time = now;
}</pre>

                <p><strong>Drift Compensation:</strong> We use bias calibration and deadband filtering. At startup, we measure the gyro bias while stationary over 200 samples (~400ms), then subtract this bias from all subsequent readings. A deadband filter zeros out readings below 0.2°/s to eliminate integration of small noise values.</p>

                <pre>static void calibrate_gyro_bias(void) {
    const int samples = 200;
    float sum = 0.0f;
    for (int i = 0; i < samples; i++) {
        mpu6050_read_raw(acceleration, gyro);
        sum += fix2float15(gyro[2]);
        sleep_ms(2);
    }
    g_gyro_bias = -(sum / samples); // Negate to compensate
    printf("Gyro Z bias calibrated to %.3f °/s\n", g_gyro_bias);
}</pre>

                <h4>LiDAR Distance Measurement</h4>
                <p>The TF-Luna uses Time-of-Flight measurement:</p>
                <p style="text-align: center; font-style: italic;">distance = (c × Δt) / 2</p>
                <p>Where c ≈ 3×10<sup>8</sup> m/s (speed of light) and Δt is round-trip time. The sensor outputs distance directly via I2C in centimeters (20-800cm range).</p>

                <p>We apply median filtering to reduce noise:</p>
                <pre>static uint16_t read_lidar_filtered(int samples) {
    uint16_t buf[8];
    int count = 0;
    for (int i = 0; i < samples && count < 8; i++) {
        tfluna_data_t data;
        if (tfluna_read_data(&lidar_sensor, &data) && data.valid && data.distance > 0) {
            buf[count++] = data.distance;
        }
        sleep_ms(2);
    }
    if (count == 0) return 0;
    return median_u16(buf, count); // Returns middle value after sorting
}</pre>

                <h4>Environmental Mapping in Polar Coordinates</h4>
                <p>For environmental mapping, we store data in polar coordinates rather than Cartesian. Each sample contains:</p>
                <ul>
                    <li><strong>θ (theta):</strong> Heading angle in degrees (0-360°)</li>
                    <li><strong>r (radius):</strong> Distance in centimeters from LiDAR</li>
                    <li><strong>T (temperature):</strong> Object temperature in °C from IR sensor</li>
                </ul>

                <p>This polar format is ideal for a robot rotating in place, as it directly maps to the robot's heading and sensor readings without requiring coordinate transformation during data collection.</p>

                <pre>typedef struct {
    float angles[MAX_MAP_SAMPLES];      // Heading in degrees
    uint16_t distances[MAX_MAP_SAMPLES]; // Distance in cm
    float temps[MAX_MAP_SAMPLES];        // Temperature in °C
    uint16_t count;                      // Number of samples
    bool active;                         // Sweep in progress
} env_map_t;</pre>

                <h4>Polar to Cartesian Conversion for Visualization</h4>
                <p>When plotting the map offline, we convert from polar to Cartesian coordinates using standard transformations:</p>
                <p style="text-align: center; font-style: italic;">x = r × cos(θ)</p>
                <p style="text-align: center; font-style: italic;">y = r × sin(θ)</p>
                <p>Where θ must be converted from degrees to radians: θ<sub>radians</sub> = θ<sub>degrees</sub> × (π / 180)</p>

                <p>Our Python visualization script implements this conversion:</p>
                <pre># Convert angles to radians for matplotlib
theta_rad = [math.radians(a) for a in angles]

# Convert polar to Cartesian
x_coords = [r * math.cos(t) for r, t in zip(distances, theta_rad)]
y_coords = [r * math.sin(t) for r, t in zip(distances, theta_rad)]</pre>

                <h4>Gait Kinematics</h4>
                <p>For a trot gait where diagonal leg pairs move together, phase relationships are:</p>
                <ul>
                    <li>Front Right + Back Left: 0° phase</li>
                    <li>Front Left + Back Right: 180° phase</li>
                </ul>

                <p>For precise control during creep gaits, we implemented inverse kinematics using the law of cosines:</p>
                <pre>void leg_position_fb(float y, int l, int s) {
    float c = 93.0f;  // Calf length (mm)
    float t = 75.0f;  // Thigh length (mm)
    float h = 40.0f;  // Height offset
    float xs = 31.8f; // Default x position
    
    float z = sqrtf(y * y + xs * xs);
    float w = sqrtf(h * h + z * z);
    float theta_h = atan2f(z, h) * 180.0f / M_PI;

    // Joint angle
    float theta_j = 135.0f - atan2f(y, xs) * 180.0f / M_PI;

    // Thigh angle using law of cosines
    float theta_t = acosf((t*t + w*w - c*c) / (2.0f*t*w)) * 180.0f/M_PI + theta_h;

    // Calf angle using law of cosines
    float theta_c = acosf((t*t + c*c - w*w) / (2.0f*t*c)) * 180.0f / M_PI;

    // Apply to specified leg...
}</pre>

                <h4>Power Calculations</h4>
                <p>Peak current draw calculation:</p>
                <p style="text-align: center; font-style: italic;">I<sub>total</sub> = I<sub>Pico</sub> + (12 × I<sub>servo</sub>) + I<sub>sensors</sub></p>
                <p style="text-align: center; font-style: italic;">I<sub>total</sub> = 200mA + (12 × 250mA) + 160mA = 3.36A</p>

                <p>We used a 5V bench supply with 2.5A maximum. Average current during typical operation was ~2A, with gaits naturally staggering servo movements to avoid simultaneous peak loads.</p>
                <p>Average power: P = V × I = 5V × 2A ≈ 10W</p>

                <h3>Software Architecture</h3>
                <p>Our software follows a modular architecture with clear separation of concerns across multiple files. The codebase is organized into functional groups for maintainability and reusability:</p>

                <div class="file-grid">
                    <div class="file-box">
                        <h4>Core Program Files</h4>
                        <pre>main.c                    - Main control loop
movement_library.c/.h     - Locomotion primitives
movementlibrary.py        - Python reference</pre>

                        <h4>Sensor Driver Modules</h4>
                        <pre>pca9685.c/.h             - Servo driver
mpu6050.c/.h             - IMU interface
tfluna_i2c.c/.h          - LiDAR interface
mlx90614.c/.h            - IR temp interface
mlx90614Config.h         - Temp config</pre>

                        <h4>Data Structures</h4>
                        <pre>mapping.c/.h             - Mapping structures
sensor_data.h            - Global sensor data</pre>
                    </div>

                    <div class="file-box">
                        <h4>Network & Communication</h4>
                        <pre>wifi.c/.h                - WiFi AP & web server
dhcpserver/              - DHCP implementation
dnsserver/               - DNS implementation
lwipopts.h               - TCP/IP options</pre>

                        <h4>Build & Configuration</h4>
                        <pre>CMakeLists.txt           - Build configuration
pico_sdk_import.cmake    - SDK integration
BUILD_WSL.md             - Build instructions
README.md                - Documentation</pre>

                        <h4>Data Output</h4>
                        <pre>sweep.csv                - Mapping data export
plot_sweep.py            - Visualization script
plot_sweep_csv.py        - CSV plotting utility</pre>
                    </div>
                </div>

                <h4>Main Control Loop Architecture</h4>
                <p>The main program coordinates all subsystems through distinct initialization and runtime phases:</p>

                <div class="control-flow">
                    <strong>Initialization Phase:</strong>
                    <pre>├── I2C bus setup (I2C1 on GPIO 2/3)
├── I2C device scanning and detection
├── PCA9685 servo driver initialization (0x40)
├── Sensor initialization
│   ├── TF-Luna LiDAR (0x10) - if detected
│   ├── MLX90614 IR temp (0x5A) - if detected
│   └── MPU6050 IMU (0x68) - if detected
├── WiFi access point startup
├── Mapping system initialization
└── Initial robot posture (stand_up, hi, xposition)</pre>

                    <strong>Main Loop:</strong>
                    <pre>├── Serial Command Processing
│   ├── Movement commands (W/A/S/D/Q/E)
│   ├── Sensor read commands (R, Y, Z)
│   ├── Special actions (H=hi, C=shuffle, B=squats)
│   └── Mapping sweep trigger (N)
├── Background Tasks (continuous)
│   ├── update_heading_from_gyro() - IMU integration
│   ├── refresh_sensor_data() - Sensor polling (~100ms)
│   └── sweep_collect_sample_if_due() - Map data collection
├── Mode Handling
│   ├── ROBOT_MODE_WIFI_CONTROL (default)
│   └── ROBOT_MODE_SCAN_APPROACH (autonomous)
├── Mapping Sweep State Machine
│   ├── Track rotation progress (0° → 360°)
│   ├── Trigger CCW rotation every 2 seconds
│   └── Export CSV when complete
└── WiFi background processing</pre>
                </div>

                <p>The modular structure allows individual components to be tested and debugged independently, while the global sensor data structure provides a single source of truth accessible by all modules. The cooperative multitasking mechanism ensures that critical background tasks (heading integration, sensor polling, WiFi servicing) continue executing even during blocking movement sequences by breaking long delays into 10ms chunks with task execution between each chunk.</p>

                <h3>Hardware/Software Tradeoffs</h3>

                <h4>Onboard vs. Offboard Processing for Map Visualization</h4>
                <p><strong>Decision:</strong> Basic sensor fusion and data collection happen onboard; visualization happens offboard via Python scripts.</p>
                <p><strong>Rationale:</strong> The Pico has limited RAM (264KB) and no GPU, making real-time graphical rendering impractical. However, the robot needs onboard data storage for autonomous decision-making. By storing data in compact polar format and exporting to CSV, we collect maximum samples while keeping memory usage minimal. We initially tried live WiFi plotting, but this repeatedly crashed due to high data rates. Offboard visualization proved more reliable while retaining autonomous capabilities.</p>

                <h4>Tethered vs. Battery Power</h4>
                <p><strong>Decision:</strong> Tethered 5V bench power supply (2.5A maximum).</p>
                <p><strong>Advantages:</strong> Unlimited runtime for extended testing, consistent voltage, no heavy battery, lower build cost.</p>
                <p><strong>Disadvantages:</strong> Cable restricts range, power supply current limit below theoretical peak draw (3.36A).</p>
                <p><strong>Mitigation:</strong> Gait patterns naturally stagger servo movements, keeping actual current around 2A average. However, movements like "shuffle" occasionally triggered the overload indicator.</p>

                <h4>Custom vs. Commercial Frame</h4>
                <p><strong>Decision:</strong> Hybrid approach - adapted open-source leg design, fully custom body.</p>
                <p><strong>Rationale:</strong> Saved approximately 40 hours of CAD work on leg geometry while maintaining full control over sensor mounting, electronics layout, and cable management through the custom 2-layer body design.</p>

                <h4>WiFi vs. Bluetooth</h4>
                <p><strong>Decision:</strong> WiFi access point mode.</p>
                <p><strong>Rationale:</strong> Higher bandwidth supports streaming all sensor data simultaneously. Serves full web interface accessible from any browser without custom app. WiFi power consumption (~50-70mA extra) is negligible compared to servo draw (2000mA average). Longer range (~30m vs. ~10m for BLE) better for testing.</p>

                <h4>Memory Management</h4>
                <p><strong>Decision:</strong> Fixed arrays, no dynamic allocation, CSV export clears map.</p>
                <p><strong>Rationale:</strong> Budget: ~100KB code/stack, ~40KB WiFi, ~30KB mapping (1000 samples), ~5KB sensors, ~89KB margin. Avoids heap fragmentation on resource-constrained system.</p>

                <h4>Sensor Fusion Approach</h4>
                <p><strong>Decision:</strong> Simple gyro integration with automatic bias calibration and 0.2°/s deadband filtering.</p>
                <p><strong>Rationale:</strong> For slow rotation during mapping sweeps, simple integration with drift mitigation is sufficient. Kalman filtering would add complexity and CPU overhead without meaningful accuracy improvement for 360° sweeps lasting ~2 minutes. Bias calibration and deadband effectively eliminate drift over these timescales.</p>

                <h4>Cooperative Multitasking vs. RTOS</h4>
                <p><strong>Decision:</strong> Custom cooperative sleep mechanism.</p>
                <p><strong>Rationale:</strong> No hard real-time requirements. Breaking delays into 10ms chunks with background task execution avoids 10-20KB RTOS overhead and learning curve. Simpler to debug and sufficient for our concurrency needs.</p>

                <h3>Patents, Copyrights, and Trademarks</h3>
                <p>This project uses open-source components with proper attribution: quadruped leg geometry from <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">Instructables</a> (Creative Commons license), Python movement library reference (MIT License), and Raspberry Pi Pico SDK (BSD-3-Clause). Sensor drivers were developed using manufacturer datasheets and public I2C specifications.</p>

                <p>Commercial products (Raspberry Pi Pico W, PCA9685, MPU6050, TF-Luna, MLX90614, MG90S servos) are trademarks of their respective manufacturers and were used as intended. Our original contributions include the custom body CAD design, C-based movement library adaptation, environmental mapping algorithm, WiFi control interface, gyro drift mitigation, autonomous sweep state machine, and visualization scripts.</p>

                <p>While quadruped locomotion and sensor fusion are extensively patented by companies like Boston Dynamics, our educational implementation uses different mechanisms (hobby servos), operates at smaller scale, and employs publicly known techniques. As an educational project for ECE 4760 at Cornell University, we properly cite all external resources, and the integration work and software architecture represent our original engineering effort.</p>
            </section>

            <!-- Remaining sections would continue here (Program/Hardware Design, Results, Conclusions, Appendix) -->
            <!-- This gives you the complete structure with proper formatting -->

        </main>
    </div>
</body>
</html>
