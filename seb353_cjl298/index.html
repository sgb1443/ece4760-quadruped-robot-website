<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>12-DOF Quadruped Robot - ECE 4760 Final Project</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Arial, sans-serif;
            line-height: 1.5;
            color: #333;
            font-size: 12px;
        }
        
        .container {
            display: flex;
            min-height: 100vh;
        }
        
        /* Sidebar Navigation */
        .sidebar {
            width: 130px;
            background-color: #2c3e50;
            color: white;
            padding: 10px;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            font-size: 11px;
        }
        
        .sidebar h2 {
            margin-bottom: 10px;
            font-size: 13px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }
        
        .sidebar nav ul {
            list-style: none;
        }
        
        .sidebar nav ul li {
            margin-bottom: 5px;
        }
        
        .sidebar nav ul li a {
            color: white;
            text-decoration: none;
            display: block;
            padding: 4px 5px;
            border-radius: 3px;
            transition: background-color 0.3s;
            font-size: 11px;
        }
        
        .sidebar nav ul li a:hover {
            background-color: #34495e;
        }
        
        .sidebar nav ul ul {
            margin-left: 10px;
            margin-top: 3px;
        }
        
        .sidebar nav ul ul li {
            margin-bottom: 3px;
        }
        
        .sidebar nav ul ul li a {
            font-size: 10px;
            padding: 3px 4px;
            color: #bdc3c7;
        }
        
        /* Main Content */
        .main-content {
            margin-left: 130px;
            padding: 20px;
            width: calc(100% - 130px);
        }
        
        h1 {
            color: #2c3e50;
            margin-bottom: 7px;
            font-size: 22px;
        }
        
        h2 {
            color: #2c3e50;
            margin-top: 18px;
            margin-bottom: 9px;
            font-size: 17px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 4px;
        }
        
        h3 {
            color: #34495e;
            margin-top: 14px;
            margin-bottom: 7px;
            font-size: 14px;
        }
        
        h4 {
            color: #34495e;
            margin-top: 11px;
            margin-bottom: 6px;
            font-size: 13px;
        }
        
        .subtitle {
            color: #7f8c8d;
            font-size: 12px;
            margin-bottom: 18px;
        }
        
        p {
            margin-bottom: 9px;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 9px;
        }
        
        ul li {
            margin-bottom: 4px;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 11px 0;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        
        .soundbite {
            background-color: #ecf0f1;
            padding: 11px;
            border-left: 4px solid #3498db;
            margin: 11px 0;
            font-style: italic;
            font-size: 12px;
        }
        
        pre {
            background: #f8f8f8;
            padding: 9px;
            border-radius: 4px;
            font-size: 10px;
            overflow-x: auto;
            border: 1px solid #ddd;
            line-height: 1.3;
        }
        
        code {
            background: #f0f0f0;
            padding: 2px 4px;
            border-radius: 3px;
            font-size: 10px;
            font-family: 'Courier New', monospace;
        }
        
        .file-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 11px;
            margin: 11px 0;
        }
        
        .file-box {
            background-color: #f5f5f5;
            padding: 9px;
            border-left: 4px solid #3498db;
        }
        
        .file-box h4 {
            margin-top: 0;
            font-size: 12px;
        }
        
        .file-box pre {
            background: white;
            margin-top: 5px;
        }
        
        .control-flow {
            background: #f8f8f8;
            padding: 11px;
            border-radius: 4px;
            border: 1px solid #ddd;
            margin: 11px 0;
        }
        
        .control-flow pre {
            background: white;
            margin: 7px 0 0 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 11px 0;
            font-size: 11px;
        }
        
        table th, table td {
            border: 1px solid #ddd;
            padding: 5px;
            text-align: left;
        }
        
        table th {
            background-color: #3498db;
            color: white;
        }
        
        table tr:nth-child(even) {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Sidebar Navigation -->
        <aside class="sidebar">
            <h2>Navigation</h2>
            <nav>
                <ul>
                    <li><a href="#intro">Introduction</a>
                        <ul>
                            <li><a href="#summary">Summary</a></li>
                            <li><a href="#what-we-did">What We Did</a></li>
                            <li><a href="#why">Why We Built This</a></li>
                        </ul>
                    </li>
                    <li><a href="#high-level">High Level Design</a>
                        <ul>
                            <li><a href="#rationale">Rationale</a></li>
                            <li><a href="#background-math">Background Math</a></li>
                            <li><a href="#logical-structure">Logical Structure</a></li>
                            <li><a href="#tradeoffs">Tradeoffs</a></li>
                            <li><a href="#ip">IP Considerations</a></li>
                        </ul>
                    </li>
                    <li><a href="#program-hardware">Program/Hardware Design</a>
                        <ul>
                            <li><a href="#overview">Overview</a></li>
                            <li><a href="#hardware">Hardware Details</a></li>
                        </ul>
                    </li>
                    <li><a href="#results">Results</a>
                        <ul>
                            <li><a href="#test-data">Test Data</a></li>
                            <li><a href="#speed">Speed</a></li>
                            <li><a href="#accuracy">Accuracy</a></li>
                            <li><a href="#safety">Safety</a></li>
                            <li><a href="#usability">Usability</a></li>
                        </ul>
                    </li>
                    <li><a href="#conclusions">Conclusions</a></li>
                    <li><a href="#appendix">Appendix</a>
                        <ul>
                            <li><a href="#permissions">Permissions</a></li>
                            <li><a href="#code">Code Listings</a></li>
                            <li><a href="#tasks">Task Distribution</a></li>
                        </ul>
                    </li>
                </ul>
            </nav>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <section id="intro">
                <h1>12-DOF Quadruped Robot with Environmental Mapping</h1>
                <p class="subtitle"><strong>Sarah Grace Brown (seb353) & Connor Lynaugh (cjl298)</strong><br>
                Cornell University - ECE 4760: Designing with Microcontrollers - Fall 2024</p>

                <div class="soundbite">
                    <strong>Project Sound Bite:</strong> A 12-DOF quadruped robot with 3 servos per leg and WiFi control that uses environmental feedback from an IMU, LiDAR, and infrared temp sensor to navigate, identify targets, and map its environment autonomously using distance and temperature data.
                </div>

                <h2 id="summary">Summary</h2>
                <p>We designed and built a 12 degree-of-freedom (3 servos per leg × 4 legs) quadruped robot controlled by a Raspberry Pi Pico W, featuring integrated environmental sensing and a wireless WiFi controller. Starting from a custom CAD body and 3D-printed frame, the robot combines mechanical engineering and embedded electrical engineering to create a platform capable of coordinated four-legged locomotion, heading determination, environmental mapping, and target detection. In order to do so, our system leverages several sensors including an IMU, a solid state LiDAR sensor, and a contact-less infrared sensor.</p>

                <img src="images/robot_assembled.jpg" alt="Assembled Quadruped Robot">
                <p style="text-align: center; color: #7f8c8d; font-size: 11px;"><em>Figure 1: Assembled quadruped robot with sensor array</em></p>

                <h2 id="what-we-did">What We Did</h2>
                <p>Our project implements a fully functional walking robot with the following key features:</p>

                <h3>Mechanical Design</h3>
                <p>We created a custom 3D-printed body and leg assemblies optimized for our specific servo operation requirements and sensor placement. After encountering flexibility issues with initial prints due to insufficient infill, we redesigned the frame to provide rigid mounting points for all four legs and the twelve MG90S servos while accommodating the electronics and wiring on the body through implementing a 2-layer mounting system, enclosing all of the servo cables. The design includes TPU grippy feet for improved traction and stability. The leg files are based off an <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">existing online design</a>, but the attachment to the body and the body itself is all custom to allow for mounting of the Raspberry Pi and sensors.</p>

                <img src="images/cad_body_top.png" alt="CAD Body Design - Top View">
                <p style="text-align: center; color: #7f8c8d; font-size: 11px;"><em>Figure 2: Custom body CAD design - top view</em></p>

                <img src="images/cad_body_iso.png" alt="CAD Body Design - Isometric View">
                <p style="text-align: center; color: #7f8c8d; font-size: 11px;"><em>Figure 3: Custom body CAD design - isometric view showing 2-layer structure</em></p>

                <img src="images/cad_assembly_top.png" alt="Full Assembly - Top View">
                <p style="text-align: center; color: #7f8c8d; font-size: 11px;"><em>Figure 4: Complete robot assembly - top view</em></p>

                <img src="images/cad_assembly_iso.png" alt="Full Assembly - Isometric View">
                <p style="text-align: center; color: #7f8c8d; font-size: 11px;"><em>Figure 5: Complete robot assembly - isometric view</em></p>

                <h3>Locomotion System</h3>
                <p>Twelve servos provide three degrees of freedom per leg (joint rotation, thigh angle, and calf angle), controlled via an Adafruit PCA9685 16-channel PWM driver over I2C. We implemented coordinated gait patterns that enable the robot to walk forward, backward, turn in place, and complete fun gestures such as waving, squatting, and shuffling. We took initial servo positions and control sequences for these movements from a <a href="https://github.com/ConnorLynaugh/Digital-Design-with-Micro-Controllers" target="_blank">Python library</a> and adapted them for C.</p>

                <h3>Sensor Integration</h3>
                <p>The robot features numerous sensors that work together to allow for autonomous navigation and environmental mapping:</p>
                <ul>
                    <li><strong>MPU6050 IMU</strong> for heading determination and orientation tracking to map the environment</li>
                    <li><strong>TF-Luna LiDAR</strong> for distance ranging (0.2-8m) to detect obstacles and map surroundings</li>
                    <li><strong>MLX90614 infrared temperature sensor</strong> for heat signature detection and thermal mapping (effective range ~30cm for human-sized targets)</li>
                </ul>

                <img src="images/wiring_diagram.png" alt="System Wiring Diagram">
                <p style="text-align: center; color: #7f8c8d; font-size: 11px;"><em>Figure 6: Complete system wiring diagram</em></p>

                <h3>Wireless Control</h3>
                <p>A WiFi-based control interface allows real-time command and control from a computer or mobile device on the local network (we used an iPad), enabling remote operation and monitoring of the robot's sensor data.</p>

                <h2 id="why">Why We Built This</h2>
                <p>We wanted to create a robot that truly interacts with the world around it. Rather than building a machine that simply responds to pre-programmed commands, we set out to design a sensing platform that could perceive, understand, and navigate complex environments autonomously while moving in a unique and challenging way. The platform we built has the potential to execute various autonomous missions with different sensor-driven objectives.</p>

                <p>Inspired by how living creatures use multiple sensory inputs simultaneously, we aimed to replicate this approach of combining data types to make decisions in a robotic platform, and have it sense in ways humans can't—in this case, thermal detection. By combining LiDAR for spatial mapping, infrared sensing for thermal detection, and IMU data for orientation tracking, our quadruped builds an understanding of its surroundings and makes intelligent decisions about movement and target identification.</p>

                <p>This project represents the intersection of our mechanical and electrical engineering backgrounds. Coordinating twelve servos in real-time while processing sensor data and maintaining wireless communication pushed us to deeply understand the integration between hardware design, software architecture, and control systems. We chose a quadruped specifically because legged locomotion is more complex than wheeled robots and presents unique challenges in stability, gait coordination, and terrain adaptability.</p>

                <p>The practical applications are compelling: quadruped robots with environmental sensing have real-world potential in search and rescue operations (thermal sensing to locate people), industrial inspection in hazardous environments, and exploration of inaccessible terrain. Or even simpler, it can act as a pet and follow you around! Building from scratch gave us the flexibility to customize the sensor suite and control functions for these use cases while demonstrating that sophisticated autonomous robotics is achievable on an affordable microcontroller platform.</p>
            </section>

            <section id="high-level">
                <h2>High Level Design</h2>

                <h3 id="rationale">Rationale and Sources of Project Idea</h3>
                <p>Our project was inspired by existing quadruped robot platforms and open-source designs available online. We began by researching quadruped locomotion and found several reference designs on platforms like Instructables and GitHub that demonstrated the feasibility of servo-based legged robots.</p>

                <p><strong>Key inspirations included:</strong></p>
                <ul>
                    <li><strong>Mechanical Design Foundation:</strong> We adapted leg geometry from an <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">open-source 3D-printed quadruped design</a> that provided a proven kinematic structure for three-DOF legs. However, we completely redesigned the body and mounting system to accommodate our specific sensor suite and electronics layout.</li>
                    <li><strong>Locomotion Control:</strong> Initial servo position sequences and gait patterns were adapted from a Python-based <a href="https://github.com/ConnorLynaugh/Digital-Design-with-Micro-Controllers" target="_blank">quadruped control library</a>. We translated these movements into C and modified them for our specific servo channel mapping and timing requirements on the Pico W.</li>
                    <li><strong>Sensor Integration Approach:</strong> The decision to combine multiple sensor modalities (LiDAR, IR temperature, IMU) was driven by our goal to create a robot with perception capabilities beyond human senses. This multi-sensory approach mirrors research in autonomous mobile robotics where sensor fusion improves environmental understanding.</li>
                    <li><strong>Platform Selection:</strong> We chose the Raspberry Pi Pico W for its balance of computational capability, built-in WiFi, and I2C support, all at a low cost point that makes the project accessible.</li>
                </ul>

                <p>The core innovation in our project is not the individual components, but rather the integration of environmental mapping capabilities with legged locomotion on a resource-constrained embedded platform, creating an affordable autonomous sensing robot.</p>

                <h3 id="background-math">Background Math</h3>

                <h4>Servo PWM Control</h4>
                <p>The MG90S servos are controlled via PWM signals from the PCA9685 controller. Standard hobby servo control operates at 50 Hz (20ms period). The pulse width determines the servo angle:</p>
                <ul>
                    <li>1ms pulse → 0°</li>
                    <li>1.5ms pulse → 90° (neutral)</li>
                    <li>2ms pulse → 180°</li>
                </ul>

                <p>The PCA9685 uses 12-bit resolution (4096 steps). To calculate pulse length in counts:</p>
                <p style="text-align: center; font-style: italic;">pulse_length = (desired_ms / 20ms) × 4096</p>
                <p>For example, 90° with a 1.5ms pulse: (1.5 / 20) × 4096 = 307 counts</p>

                <p>Our servo control function maps angles to PWM values:</p>
                <pre>uint16_t angle_to_pwm(float angle) {
    // Map 0-180° to approximately 150-600 counts (calibrated for MG90S)
    return (uint16_t)(SERVOMIN + (angle / 180.0) * (SERVOMAX - SERVOMIN));
}

void set_servo_angle(uint8_t channel, float angle) {
    uint16_t pwm = angle_to_pwm(angle);
    pca9685_set_pwm(channel, 0, pwm);
}</pre>

                <h4>IMU Heading Calculation</h4>
                <p>The MPU6050 provides gyroscope data that we use for heading determination. We integrate angular velocity over time to track orientation:</p>
                <p style="text-align: center; font-style: italic;">heading(t) = heading(t-1) + ω<sub>z</sub> × Δt</p>
                <p>Where ω<sub>z</sub> is angular velocity around the z-axis (yaw rate) measured in degrees per second, and Δt is the time interval.</p>

                <pre>static inline void update_heading_from_gyro(void) {
    uint64_t now = time_us_64();
    float dt = (now - g_last_gyro_time) / 1000000.0f;
    if (dt < 0.002f) return;

    mpu6050_read_raw(acceleration, gyro);
    float gz = fix2float15(gyro[2]);
    gz += g_gyro_bias; // Apply bias compensation

    // Deadband filter to eliminate drift from small noise
    if (fabsf(gz) < 0.2f) gz = 0.0f;

    g_gyro_heading += gz * dt;

    // Keep heading in 0-360° range
    while (g_gyro_heading < 0) g_gyro_heading += 360.0f;
    while (g_gyro_heading >= 360.0f) g_gyro_heading -= 360.0f;

    g_last_gyro_time = now;
}</pre>

                <p><strong>Drift Compensation:</strong> We use bias calibration and deadband filtering. At startup, we measure the gyro bias while stationary over 200 samples (~400ms), then subtract this bias from all subsequent readings. A deadband filter zeros out readings below 0.2°/s to eliminate integration of small noise values.</p>

                <pre>static void calibrate_gyro_bias(void) {
    const int samples = 200;
    float sum = 0.0f;
    for (int i = 0; i < samples; i++) {
        mpu6050_read_raw(acceleration, gyro);
        sum += fix2float15(gyro[2]);
        sleep_ms(2);
    }
    g_gyro_bias = -(sum / samples); // Negate to compensate
    printf("Gyro Z bias calibrated to %.3f °/s\n", g_gyro_bias);
}</pre>

                <h4>LiDAR Distance Measurement</h4>
                <p>The TF-Luna uses Time-of-Flight measurement:</p>
                <p style="text-align: center; font-style: italic;">distance = (c × Δt) / 2</p>
                <p>Where c ≈ 3×10<sup>8</sup> m/s (speed of light) and Δt is round-trip time. The sensor outputs distance directly via I2C in centimeters (20-800cm range).</p>

                <p>We apply median filtering to reduce noise:</p>
                <pre>static uint16_t read_lidar_filtered(int samples) {
    uint16_t buf[8];
    int count = 0;
    for (int i = 0; i < samples && count < 8; i++) {
        tfluna_data_t data;
        if (tfluna_read_data(&lidar_sensor, &data) && data.valid && data.distance > 0) {
            buf[count++] = data.distance;
        }
        sleep_ms(2);
    }
    if (count == 0) return 0;
    return median_u16(buf, count); // Returns middle value after sorting
}</pre>

                <h4>Environmental Mapping in Polar Coordinates</h4>
                <p>For environmental mapping, we store data directly in polar coordinates. Each sample contains:</p>
                <ul>
                    <li><strong>θ (theta):</strong> Heading angle in degrees (0-360°) from the IMU</li>
                    <li><strong>r (radius):</strong> Distance in centimeters from LiDAR</li>
                    <li><strong>T (temperature):</strong> Object temperature in °C from IR sensor</li>
                </ul>

                <p>This polar format is ideal for a robot rotating in place, as it directly maps to the robot's heading and sensor readings without requiring coordinate transformation during data collection.</p>

                <pre>typedef struct {
    float angles[MAX_MAP_SAMPLES];      // Heading in degrees
    uint16_t distances[MAX_MAP_SAMPLES]; // Distance in cm
    float temps[MAX_MAP_SAMPLES];        // Temperature in °C
    uint16_t count;                      // Number of samples
    bool active;                         // Sweep in progress
} env_map_t;</pre>

                <p>During a 360-degree sweep, samples are collected at approximately 100 Hz. As the robot rotates, each sample is stored with its current heading from the gyro integration, the distance reading from the LiDAR, and the temperature from the IR sensor. The data remains in polar form and is exported to CSV for visualization.</p>

                <h4>Polar Visualization</h4>
                <p>Our Python visualization script plots the data directly in polar coordinates using matplotlib's polar projection. The angles are converted from degrees to radians for matplotlib compatibility:</p>
                <pre># Convert angles to radians for matplotlib polar plot
theta_rad = [math.radians(a) for a in angles]

# Create polar scatter plot
ax.scatter(theta_rad, distances, c=temps_f, cmap='plasma')</pre>

                <p>We also create a top-down Cartesian view by converting polar to Cartesian coordinates:</p>
                <p style="text-align: center; font-style: italic;">x = r × cos(θ)</p>
                <p style="text-align: center; font-style: italic;">y = r × sin(θ)</p>

                <pre># Convert polar to Cartesian for top-down view
x_coords = [r * math.cos(t) for r, t in zip(distances, theta_rad)]
y_coords = [r * math.sin(t) for r, t in zip(distances, theta_rad)]</pre>

                <h4>Gait Kinematics</h4>
                <p>For a trot gait where diagonal leg pairs move together, phase relationships are:</p>
                <ul>
                    <li>Front Right + Back Left: 0° phase</li>
                    <li>Front Left + Back Right: 180° phase</li>
                </ul>

                <p>For precise control during creep gaits, we implemented inverse kinematics using the law of cosines:</p>
                <pre>void leg_position_fb(float y, int l, int s) {
    float c = 93.0f;  // Calf length (mm)
    float t = 75.0f;  // Thigh length (mm)
    float h = 40.0f;  // Height offset
    float xs = 31.8f; // Default x position
    
    float z = sqrtf(y * y + xs * xs);
    float w = sqrtf(h * h + z * z);
    float theta_h = atan2f(z, h) * 180.0f / M_PI;

    // Joint angle
    float theta_j = 135.0f - atan2f(y, xs) * 180.0f / M_PI;

    // Thigh angle using law of cosines
    float theta_t = acosf((t*t + w*w - c*c) / (2.0f*t*w)) * 180.0f/M_PI + theta_h;

    // Calf angle using law of cosines
    float theta_c = acosf((t*t + c*c - w*w) / (2.0f*t*c)) * 180.0f / M_PI;

    // Apply to specified leg...
}</pre>

              <h4>Power Calculations</h4>
<p>Peak current draw calculation:</p>
<p style="text-align: center; font-style: italic;">I<sub>total</sub> = I<sub>Pico</sub> + (12 × I<sub>servo</sub>) + I<sub>sensors</sub></p>
<p style="text-align: center; font-style: italic;">I<sub>total</sub> = 200mA + (12 × 250mA) + 160mA = 3.36A</p>

<p>We used a 5V bench supply with 2.5A maximum. Average current during typical operation was ~2A, with gaits naturally staggering servo movements to avoid simultaneous peak loads. Further explanation of the choice to use bench power and the current limitations is discussed in detail in the Hardware/Software Tradeoffs section below.</p>

<p>Average power during typical operation: P = V × I = 5V × 2A ≈ 10W</p>

                <h3 id="logical-structure">Logical Structure</h3>
                <p>Our software follows a modular architecture with clear separation of concerns across multiple files. The codebase is organized into functional groups for maintainability and reusability:</p>

                <div class="file-grid">
                    <div class="file-box">
                        <h4>Core Program Files</h4>
                        <pre>main.c                    - Main control loop
movement_library.c/.h     - Locomotion primitives
movementlibrary.py        - Python reference</pre>

                        <h4>Sensor Driver Modules</h4>
                        <pre>pca9685.c/.h             - Servo driver
mpu6050.c/.h             - IMU interface
tfluna_i2c.c/.h          - LiDAR interface
mlx90614.c/.h            - IR temp interface
mlx90614Config.h         - Temp config</pre>

                        <h4>Data Structures</h4>
                        <pre>mapping.c/.h             - Mapping structures
sensor_data.h            - Global sensor data</pre>
                    </div>

                    <div class="file-box">
                        <h4>Network & Communication</h4>
                        <pre>wifi.c/.h                - WiFi AP & web server
dhcpserver/              - DHCP implementation
dnsserver/               - DNS implementation
lwipopts.h               - TCP/IP options</pre>

                        <h4>Build & Configuration</h4>
                        <pre>CMakeLists.txt           - Build configuration
pico_sdk_import.cmake    - SDK integration
BUILD_WSL.md             - Build instructions
README.md                - Documentation</pre>

                        <h4>Data Output</h4>
                        <pre>sweep.csv                - Mapping data export
plot_sweep.py            - Visualization script
plot_sweep_csv.py        - CSV plotting utility</pre>
                    </div>
                </div>

                <h4>Main Control Loop Architecture</h4>
                <p>The main program coordinates all subsystems through distinct initialization and runtime phases. This architecture ensures that all sensors, communication systems, and movement capabilities are properly initialized before entering the main operational loop.</p>

                <div class="control-flow">
                    <strong>Initialization Phase:</strong>
                    <pre>├── I2C bus setup (I2C1 on GPIO 2/3)
├── I2C device scanning and detection
├── PCA9685 servo driver initialization (0x40)
├── Sensor initialization
│   ├── TF-Luna LiDAR (0x10) - if detected
│   ├── MLX90614 IR temp (0x5A) - if detected
│   └── MPU6050 IMU (0x68) - if detected
├── WiFi access point startup
├── Mapping system initialization
└── Initial robot posture (stand_up, hi, xposition)</pre>

                    <strong>Main Loop:</strong>
                    <pre>├── Serial Command Processing
│   ├── Movement commands (W/A/S/D/Q/E)
│   ├── Sensor read commands (R, Y, Z)
│   ├── Special actions (H=hi, C=shuffle, B=squats)
│   └── Mapping sweep trigger (N)
├── Background Tasks (continuous)
│   ├── update_heading_from_gyro() - IMU integration
│   ├── refresh_sensor_data() - Sensor polling (~100ms)
│   └── sweep_collect_sample_if_due() - Map data collection
├── Mode Handling
│   ├── ROBOT_MODE_WIFI_CONTROL (default)
│   └── ROBOT_MODE_SCAN_APPROACH (autonomous)
├── Mapping Sweep State Machine
│   ├── Track rotation progress (0° → 360°)
│   ├── Trigger CCW rotation every 2 seconds
│   └── Export CSV when complete
└── WiFi background processing</pre>
                </div>

                <p>The modular structure allows individual components to be tested and debugged independently, while the global sensor data structure provides a single source of truth accessible by all modules. The cooperative multitasking mechanism ensures that critical background tasks (heading integration, sensor polling, WiFi servicing) continue executing even during blocking movement sequences by breaking long delays into 10ms chunks with task execution between each chunk.</p>

                <h3 id="tradeoffs">Hardware/Software Tradeoffs</h3>

                <h4>Onboard vs. Offboard Processing for Map Visualization</h4>
                <p><strong>Decision:</strong> Basic sensor fusion and data collection happen onboard; visualization happens offboard via Python scripts.</p>
                <p><strong>Rationale:</strong> The Pico has limited RAM (264KB) and no GPU, making real-time graphical rendering impractical. However, the robot needs onboard data storage for autonomous decision-making. By storing data in compact polar format and exporting to CSV, we collect maximum samples while keeping memory usage minimal. We initially tried live WiFi plotting, but this repeatedly crashed due to high data rates. Offboard visualization proved more reliable while retaining autonomous capabilities.</p>

                <h4>Tethered vs. Battery Power</h4>
                <p><strong>Decision:</strong> Tethered 5V bench power supply (2.5A maximum).</p>
                <p><strong>Advantages:</strong> Unlimited runtime for extended testing and mapping sessions, consistent voltage regardless of load, no heavy battery weight affecting balance and gait stability, lower total build cost (~$111 vs. $150+ with suitable battery and charging system), no charging downtime between test sessions.</p>
                <p><strong>Disadvantages:</strong> Cable restricts range to ~2-3 meters and can interfere with motion during turns, power supply current limit (2.5A) below theoretical peak draw (3.36A) means risk of brownouts during simultaneous servo movements, tether creates dependency on nearby wall outlet limiting deployment scenarios.</p>
                <p><strong>Mitigation and Reality:</strong> Gait patterns naturally stagger servo movements - during a trot gait, only 6 servos (diagonal pair) move simultaneously under heavy load, keeping typical current around 2A average. Simple movements like forward, backward, and gentle turns stay well within the 2.5A limit. However, complex movements like "shuffle" that activate many servos simultaneously did occasionally trigger the overload indicator on our bench supply, causing brief voltage dips. For this educational project, tethered operation was acceptable as our test area was controlled and we prioritized unlimited runtime for iterative testing over mobility. A production version would require a 3S LiPo battery (11.1V, 2200mAh minimum) with a 5V buck converter rated for 4A continuous.</p>

                <h4>Custom vs. Commercial Frame</h4>
                <p><strong>Decision:</strong> Hybrid approach - adapted open-source leg design, fully custom body.</p>
                <p><strong>Rationale:</strong> Saved approximately 40 hours of CAD work on leg geometry while maintaining full control over sensor mounting, electronics layout, and cable management through the custom 2-layer body design. The leg kinematics were proven and well-documented, eliminating risk of mechanical design errors. Our custom body allowed precise positioning of the LiDAR (forward-facing), IR sensor (forward at specific height), and IMU (centered for accurate heading), which would be impossible to retrofit onto an existing commercial quadruped frame.</p>

                <h4>WiFi vs. Bluetooth</h4>
                <p><strong>Decision:</strong> WiFi access point mode.</p>
                <p><strong>Rationale:</strong> Higher bandwidth supports streaming all sensor data simultaneously (heading, distance, temperature at 10 Hz). Serves full web interface accessible from any browser without custom app development or installation. WiFi power consumption (~50-70mA extra) is negligible compared to servo draw (2000mA average). Longer range (~30m vs. ~10m for BLE) better for outdoor testing. AP mode creates captive portal effect - when iOS/Android devices connect, they automatically open the control page without requiring manual URL entry.</p>

                <h4>Memory Management</h4>
                <p><strong>Decision:</strong> Fixed arrays, no dynamic allocation, CSV export clears map buffer for reuse.</p>
                <p><strong>Rationale:</strong> Memory budget breakdown: ~100KB code/stack, ~40KB WiFi/lwIP, ~30KB mapping arrays (1000 samples × 10 bytes), ~5KB sensor buffers, ~89KB margin. Fixed allocation at compile time prevents heap fragmentation issues on resource-constrained system. Maximum map size of 1000 samples provides sufficient resolution for room-scale mapping (~10 samples per degree during 360° sweep). After CSV export, the map buffer is reset for the next sweep without memory leaks.</p>

                <h4>Sensor Fusion Approach</h4>
                <p><strong>Decision:</strong> Simple gyro integration with automatic bias calibration and 0.2°/s deadband filtering.</p>
                <p><strong>Rationale:</strong> For slow rotation during mapping sweeps (180°/min, 3°/s angular velocity), simple integration with drift mitigation is sufficient. Our testing showed accumulated drift of only 5-10° over full 360° sweep (~2 minutes). Kalman filtering would require accelerometer fusion, add ~2KB code, consume 5-10% CPU, and provide minimal accuracy improvement for this use case. The bias calibration (200 samples at startup) and deadband (0.2°/s threshold) effectively eliminate drift over mapping timescales. For applications requiring absolute heading over hours, a magnetometer would be necessary, but for our 2-4 minute mapping sweeps, gyro-only tracking proved adequate.</p>

                <h4>Cooperative Multitasking vs. RTOS</h4>
                <p><strong>Decision:</strong> Custom cooperative sleep mechanism breaking delays into 10ms chunks.</p>
                <p><strong>Rationale:</strong> No hard real-time requirements exist in our system. Movement sequences can tolerate ~10ms jitter without visible impact on gait quality. WiFi servicing occurs during these 10ms windows, preventing connection timeouts. Sensor polling (100ms period) and heading integration (2ms period) fit comfortably within cooperative scheduling. RTOS (FreeRTOS) would add 10-20KB code overhead, 2-4KB RAM for task stacks, steeper learning curve, and debugging complexity. Our cooperative approach uses <1KB additional code and is trivial to debug with printf statements. For future work requiring multiple simultaneous autonomous behaviors, RTOS might become beneficial, but for current functionality, cooperative multitasking proved simpler and sufficient.</p>

                <h3 id="ip">Patents, Copyrights, and Trademarks</h3>
                <p>This project uses open-source components with proper attribution: quadruped leg geometry from <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">Instructables</a> (Creative Commons license), Python movement library reference (MIT License), and Raspberry Pi Pico SDK (BSD-3-Clause). Sensor drivers were developed using manufacturer datasheets and public I2C specifications.</p>

                <p>Commercial products (Raspberry Pi Pico W, PCA9685, MPU6050, TF-Luna, MLX90614, MG90S servos) are trademarks of their respective manufacturers and were used as intended. Our original contributions include the custom body CAD design, C-based movement library adaptation, environmental mapping algorithm, WiFi control interface, gyro drift mitigation, autonomous sweep state machine, and visualization scripts.</p>

                <p>While quadruped locomotion and sensor fusion are extensively patented by companies like Boston Dynamics, our educational implementation uses different mechanisms (hobby servos vs. electric/hydraulic actuators), operates at smaller scale (30cm vs. 1m+ body length), and employs publicly known techniques (PWM control, I2C communication, gyro integration). As an educational project for ECE 4760 at Cornell University, we properly cite all external resources, and the integration work and software architecture represent our original engineering effort. No NDAs were required as all components were purchased through standard retail channels with publicly available documentation.</p>
            </section>

            <!-- Additional sections would continue here -->
            <section id="program-hardware">
                <h2>Program/Hardware Design</h2>
                <p><em>[Content continues with detailed software/hardware implementation...]</em></p>
            </section>

            <section id="results">
                <h2>Results</h2>
                <p><em>[Content continues with test data, accuracy, speed, safety, and usability results...]</em></p>
            </section>

            <section id="conclusions">
                <h2>Conclusions</h2>
                <p><em>[Content continues with analysis and future improvements...]</em></p>
            </section>

            <section id="appendix">
                <h2>Appendix</h2>
                <p><em>[Content continues with permissions, code listings, and task distribution...]</em></p>
            </section>

        </main>
    </div>
</body>
</html>
