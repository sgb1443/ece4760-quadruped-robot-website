<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>12-DOF Quadruped Robot - ECE 4760 Final Project</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
        }
        
        .container {
            display: flex;
            min-height: 100vh;
        }
        
        /* Sidebar Navigation */
        .sidebar {
            width: 250px;
            background-color: #2c3e50;
            color: white;
            padding: 20px;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
        }
        
        .sidebar h2 {
            margin-bottom: 20px;
            font-size: 18px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        
        .sidebar nav ul {
            list-style: none;
        }
        
        .sidebar nav ul li {
            margin-bottom: 10px;
        }
        
        .sidebar nav ul li a {
            color: white;
            text-decoration: none;
            display: block;
            padding: 8px 10px;
            border-radius: 4px;
            transition: background-color 0.3s;
        }
        
        .sidebar nav ul li a:hover {
            background-color: #34495e;
        }
        
        /* Main Content */
        .main-content {
            margin-left: 250px;
            padding: 40px;
            width: calc(100% - 250px);
        }
        
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 32px;
        }
        
        h2 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 24px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }
        
        h3 {
            color: #34495e;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 20px;
        }
        
        .subtitle {
            color: #7f8c8d;
            font-size: 18px;
            margin-bottom: 30px;
        }
        
        p {
            margin-bottom: 15px;
        }
        
        ul {
            margin-left: 30px;
            margin-bottom: 15px;
        }
        
        ul li {
            margin-bottom: 8px;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 20px 0;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        
        .soundbite {
            background-color: #ecf0f1;
            padding: 20px;
            border-left: 4px solid #3498db;
            margin: 20px 0;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Sidebar Navigation -->
        <aside class="sidebar">
            <h2>Navigation</h2>
            <nav>
                <ul>
                    <li><a href="#intro">Introduction</a></li>
                    <li><a href="#high-level">High Level Design</a></li>
                    <li><a href="#program-hardware">Program/Hardware Design</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#conclusions">Conclusions</a></li>
                    <li><a href="#appendix">Appendix</a></li>
                </ul>
            </nav>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <section id="intro">
                <h1>12-DOF Quadruped Robot with Environmental Mapping</h1>
                <p class="subtitle"><strong>Sarah Grace Brown (seb353) & Connor Lynaugh (cjl298)</strong><br>
                Cornell University - ECE 4760: Designing with Microcontrollers - Fall 2024</p>

                <div class="soundbite">
                    <strong>Project Sound Bite:</strong> A 12-DOF quadruped robot with 3 servos per leg and WiFi control that uses environmental feedback from an IMU, LiDAR, and infrared temp sensor to navigate, identify targets, and map its environment autonomously using distance and temperature data.
                </div>

                <h2>Summary</h2>
                <p>We designed and built a 12 degree-of-freedom (3 servos per leg × 4 legs) quadruped robot controlled by a Raspberry Pi Pico W, featuring integrated environmental sensing and a wireless WiFi controller. Starting from a custom CAD body and 3D-printed frame, the robot combines mechanical engineering and embedded electrical engineering to create a platform capable of coordinated four-legged locomotion, heading determination, environmental mapping, and target detection. In order to do so, our system leverages several sensors including an IMU, a solid state LiDAR sensor, and a contact-less infrared sensor.</p>

                <img src="images/robot_assembled.jpg" alt="Assembled Quadruped Robot">
                <p style="text-align: center; color: #7f8c8d; font-size: 14px;"><em>Figure 1: Assembled quadruped robot with sensor array</em></p>

                <h2>What We Did</h2>
                <p>Our project implements a fully functional walking robot with the following key features:</p>

                <h3>Mechanical Design</h3>
                <p>We created a custom 3D-printed body and leg assemblies optimized for our specific servo operation requirements and sensor placement. After encountering flexibility issues with initial prints due to insufficient infill, we redesigned the frame to provide rigid mounting points for all four legs and the twelve MG90S servos while accommodating the electronics and wiring on the body through implementing a 2-layer mounting system, enclosing all of the servo cables. The design includes TPU grippy feet for improved traction and stability. The leg files are based off an <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">existing online design</a>, but the attachment to the body and the body itself is all custom to allow for mounting of the Raspberry Pi and sensors.</p>

                <img src="images/cad_body_top.png" alt="CAD Body Design - Top View">
                <p style="text-align: center; color: #7f8c8d; font-size: 14px;"><em>Figure 2: Custom body CAD design - top view</em></p>

                <img src="images/cad_body_iso.png" alt="CAD Body Design - Isometric View">
                <p style="text-align: center; color: #7f8c8d; font-size: 14px;"><em>Figure 3: Custom body CAD design - isometric view showing 2-layer structure</em></p>

                <img src="images/cad_assembly_top.png" alt="Full Assembly - Top View">
                <p style="text-align: center; color: #7f8c8d; font-size: 14px;"><em>Figure 4: Complete robot assembly - top view</em></p>

                <img src="images/cad_assembly_iso.png" alt="Full Assembly - Isometric View">
                <p style="text-align: center; color: #7f8c8d; font-size: 14px;"><em>Figure 5: Complete robot assembly - isometric view</em></p>

                <h3>Locomotion System</h3>
                <p>Twelve servos provide three degrees of freedom per leg (joint rotation, thigh angle, and calf angle), controlled via an Adafruit PCA9685 16-channel PWM driver over I2C. We implemented coordinated gait patterns that enable the robot to walk forward, backward, turn in place, and complete fun gestures such as waving, squatting, and shuffling. We took initial servo positions and control sequences for these movements from a <a href="https://github.com/ConnorLynaugh/Digital-Design-with-Micro-Controllers" target="_blank">Python library</a> and adapted them for C.</p>

                <h3>Sensor Integration</h3>
                <p>The robot features numerous sensors that work together to allow for autonomous navigation and environmental mapping:</p>
                <ul>
                    <li><strong>MPU6050 IMU</strong> for heading determination and orientation tracking to map the environment</li>
                    <li><strong>TF-Luna LiDAR</strong> for distance ranging (0.2-8m) to detect obstacles and map surroundings</li>
                    <li><strong>MLX90614 infrared temperature sensor</strong> for heat signature detection and thermal mapping (effective range ~30cm for human-sized targets)</li>
                </ul>

                <img src="images/wiring_diagram.png" alt="System Wiring Diagram">
                <p style="text-align: center; color: #7f8c8d; font-size: 14px;"><em>Figure 6: Complete system wiring diagram</em></p>

                <h3>Wireless Control</h3>
                <p>A WiFi-based control interface allows real-time command and control from a computer or mobile device on the local network (we used an iPad), enabling remote operation and monitoring of the robot's sensor data.</p>

                <h2>Why We Built This</h2>
                <p>We wanted to create a robot that truly interacts with the world around it. Rather than building a machine that simply responds to pre-programmed commands, we set out to design a sensing platform that could perceive, understand, and navigate complex environments autonomously while moving in a unique and challenging way. The platform we built has the potential to execute various autonomous missions with different sensor-driven objectives.</p>

                <p>Inspired by how living creatures use multiple sensory inputs simultaneously, we aimed to replicate this approach of combining data types to make decisions in a robotic platform, and have it sense in ways humans can't—in this case, thermal detection. By combining LiDAR for spatial mapping, infrared sensing for thermal detection, and IMU data for orientation tracking, our quadruped builds an understanding of its surroundings and makes intelligent decisions about movement and target identification.</p>

                <p>This project represents the intersection of our mechanical and electrical engineering backgrounds. Coordinating twelve servos in real-time while processing sensor data and maintaining wireless communication pushed us to deeply understand the integration between hardware design, software architecture, and control systems. We chose a quadruped specifically because legged locomotion is more complex than wheeled robots and presents unique challenges in stability, gait coordination, and terrain adaptability.</p>

                <p>The practical applications are compelling: quadruped robots with environmental sensing have real-world potential in search and rescue operations (thermal sensing to locate people), industrial inspection in hazardous environments, and exploration of inaccessible terrain. Or even simpler, it can act as a pet and follow you around! Building from scratch gave us the flexibility to customize the sensor suite and control functions for these use cases while demonstrating that sophisticated autonomous robotics is achievable on an affordable microcontroller platform.</p>
            </section>

           <section id="high-level">
    <h2>High Level Design</h2>

    <h3>Rationale and Sources of Project Idea</h3>
    <p>Our project was inspired by existing quadruped robot platforms and open-source designs available online. We began by researching quadruped locomotion and found several reference designs on platforms like Instructables and GitHub that demonstrated the feasibility of servo-based legged robots.</p>

    <p><strong>Key inspirations included:</strong></p>
    <ul>
        <li><strong>Mechanical Design Foundation:</strong> We adapted leg geometry from an <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">open-source 3D-printed quadruped design</a> that provided a proven kinematic structure for three-DOF legs. However, we completely redesigned the body and mounting system to accommodate our specific sensor suite and electronics layout.</li>
        <li><strong>Locomotion Control:</strong> Initial servo position sequences and gait patterns were adapted from a Python-based <a href="https://github.com/ConnorLynaugh/Digital-Design-with-Micro-Controllers" target="_blank">quadruped control library</a>. We translated these movements into C and modified them for our specific servo channel mapping and timing requirements on the Pico W.</li>
        <li><strong>Sensor Integration Approach:</strong> The decision to combine multiple sensor modalities (LiDAR, IR temperature, IMU) was driven by our goal to create a robot with perception capabilities beyond human senses. This multi-sensory approach mirrors research in autonomous mobile robotics where sensor fusion improves environmental understanding.</li>
        <li><strong>Platform Selection:</strong> We chose the Raspberry Pi Pico W for its balance of computational capability, built-in WiFi, and I2C support, all at a low cost point that makes the project accessible.</li>
    </ul>

    <p>The core innovation in our project is not the individual components, but rather the integration of environmental mapping capabilities with legged locomotion on a resource-constrained embedded platform, creating an affordable autonomous sensing robot.</p>

    <h3>Background Math</h3>

    <h4>Servo PWM Control</h4>
    <p>The MG90S servos are controlled via PWM signals from the PCA9685 controller. Standard hobby servo control operates at 50 Hz (20ms period). The pulse width determines the servo angle:</p>
    <ul>
        <li>1ms pulse → 0°</li>
        <li>1.5ms pulse → 90° (neutral)</li>
        <li>2ms pulse → 180°</li>
    </ul>

    <p>The PCA9685 uses 12-bit resolution (4096 steps). To calculate pulse length in counts:</p>
    <p style="text-align: center; font-style: italic;">pulse_length = (desired_ms / 20ms) × 4096</p>
    <p>For example, 90° with a 1.5ms pulse: (1.5 / 20) × 4096 = 307 counts</p>

    <h4>Environmental Mapping in Polar Coordinates</h4>
    <p>For environmental mapping, we store data in polar coordinates rather than Cartesian. Each sample contains:</p>
    <ul>
        <li><strong>θ (theta):</strong> Heading angle in degrees (0-360°)</li>
        <li><strong>r (radius):</strong> Distance in centimeters from LiDAR</li>
        <li><strong>T (temperature):</strong> Object temperature in °C from IR sensor</li>
    </ul>

    <p>This polar format is ideal for a robot rotating in place, as it directly maps to the robot's heading and sensor readings without requiring coordinate transformation during data collection.</p>

    <h4>Polar to Cartesian Conversion for Visualization</h4>
    <p>When plotting the map, we convert from polar to Cartesian coordinates using standard transformations:</p>
    <p style="text-align: center; font-style: italic;">x = r × cos(θ)</p>
    <p style="text-align: center; font-style: italic;">y = r × sin(θ)</p>
    <p>Where θ must be converted from degrees to radians: θ<sub>radians</sub> = θ<sub>degrees</sub> × (π / 180)</p>

    <h3>Software Architecture</h3>
    <p>Our software follows a modular architecture with clear separation of concerns across multiple files. The codebase is organized into functional groups for maintainability and reusability:</p>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div style="background-color: #f5f5f5; padding: 15px; border-left: 4px solid #3498db;">
            <h4 style="margin-top: 0;">Core Program Files</h4>
            <pre style="background: white; padding: 10px; border-radius: 4px; font-size: 13px; overflow-x: auto;">
main.c                    - Main control loop
movement_library.c/.h     - Locomotion primitives
movementlibrary.py        - Python reference</pre>

            <h4>Sensor Driver Modules</h4>
            <pre style="background: white; padding: 10px; border-radius: 4px; font-size: 13px; overflow-x: auto;">
pca9685.c/.h             - Servo driver
mpu6050.c/.h             - IMU interface
tfluna_i2c.c/.h          - LiDAR interface
mlx90614.c/.h            - IR temp interface
mlx90614Config.h         - Temp config</pre>

            <h4>Data Structures</h4>
            <pre style="background: white; padding: 10px; border-radius: 4px; font-size: 13px; overflow-x: auto;">
mapping.c/.h             - Mapping structures
sensor_data.h            - Global sensor data</pre>
        </div>

        <div style="background-color: #f5f5f5; padding: 15px; border-left: 4px solid #3498db;">
            <h4 style="margin-top: 0;">Network & Communication</h4>
            <pre style="background: white; padding: 10px; border-radius: 4px; font-size: 13px; overflow-x: auto;">
wifi.c/.h                - WiFi AP & web server
dhcpserver/              - DHCP implementation
dnsserver/               - DNS implementation
lwipopts.h               - TCP/IP options</pre>

            <h4>Build & Configuration</h4>
            <pre style="background: white; padding: 10px; border-radius: 4px; font-size: 13px; overflow-x: auto;">
CMakeLists.txt           - Build configuration
pico_sdk_import.cmake    - SDK integration
BUILD_WSL.md             - Build instructions
README.md                - Documentation</pre>

            <h4>Data Output</h4>
            <pre style="background: white; padding: 10px; border-radius: 4px; font-size: 13px; overflow-x: auto;">
sweep.csv                - Mapping data export
plot_sweep.py            - Visualization script
plot_sweep_csv.py        - CSV plotting utility</pre>
        </div>
    </div>

    <p>The modular structure allows individual components to be tested and debugged independently, while the global sensor data structure provides a single source of truth accessible by all modules. Detailed implementation of each module is covered in the Program/Hardware Design section.</p>

    <h3>Hardware/Software Tradeoffs</h3>

    <h4>Onboard vs. Offboard Processing for Map Visualization</h4>
    <p><strong>Decision:</strong> Basic sensor fusion and data collection happen onboard; visualization happens offboard via Python scripts.</p>
    <p><strong>Rationale:</strong> The Pico has limited RAM (264KB) and no GPU, making real-time graphical rendering impractical. However, the robot needs onboard data storage for autonomous decision-making. By storing data in compact polar format and exporting to CSV, we collect maximum samples while keeping memory usage minimal. We initially tried live WiFi plotting, but this repeatedly crashed due to high data rates. Offboard visualization proved more reliable while retaining autonomous capabilities.</p>

    <h4>Tethered vs. Battery Power</h4>
    <p><strong>Decision:</strong> Tethered 5V bench power supply (2.5A maximum).</p>
    <p><strong>Advantages:</strong> Unlimited runtime for extended testing, consistent voltage, no heavy battery, lower build cost.</p>
    <p><strong>Disadvantages:</strong> Cable restricts range, power supply current limit below theoretical peak draw (3.36A).</p>
    <p><strong>Mitigation:</strong> Gait patterns naturally stagger servo movements, keeping actual current around 2A average. However, movements like "shuffle" occasionally triggered the overload indicator.</p>

    <h4>Custom vs. Commercial Frame</h4>
    <p><strong>Decision:</strong> Hybrid approach - adapted open-source leg design, fully custom body.</p>
    <p><strong>Rationale:</strong> Saved approximately 40 hours of CAD work on leg geometry while maintaining full control over sensor mounting, electronics layout, and cable management through the custom 2-layer body design.</p>

    <h4>WiFi vs. Bluetooth</h4>
    <p><strong>Decision:</strong> WiFi access point mode.</p>
    <p><strong>Rationale:</strong> Higher bandwidth supports streaming all sensor data simultaneously. Serves full web interface accessible from any browser without custom app. WiFi power consumption (~50-70mA extra) is negligible compared to servo draw (2000mA average). Longer range (~30m vs. ~10m for BLE) better for testing.</p>

    <h4>Sensor Fusion Approach</h4>
    <p><strong>Decision:</strong> Simple gyro integration with automatic bias calibration and 0.2°/s deadband filtering.</p>
    <p><strong>Rationale:</strong> For slow rotation during mapping sweeps, simple integration with drift mitigation is sufficient. Kalman filtering would add complexity and CPU overhead without meaningful accuracy improvement for 360° sweeps lasting ~2 minutes. Bias calibration and deadband effectively eliminate drift over these timescales.</p>

    <h4>Cooperative Multitasking vs. RTOS</h4>
    <p><strong>Decision:</strong> Custom cooperative sleep mechanism.</p>
    <p><strong>Rationale:</strong> No hard real-time requirements. Breaking delays into 10ms chunks with background task execution avoids 10-20KB RTOS overhead and learning curve. Simpler to debug and sufficient for our concurrency needs.</p>

    <h3>Patents, Copyrights, and Trademarks</h3>
    <p>This project uses open-source components with proper attribution: quadruped leg geometry from <a href="https://www.instructables.com/3D-Printed-Raspberry-Pi-Spider-Robot-Platform/" target="_blank">Instructables</a> (Creative Commons license), Python movement library reference (MIT License), and Raspberry Pi Pico SDK (BSD-3-Clause). Sensor drivers were developed using manufacturer datasheets and public I2C specifications.</p>

    <p>Commercial products (Raspberry Pi Pico W, PCA9685, MPU6050, TF-Luna, MLX90614, MG90S servos) are trademarks of their respective manufacturers and were used as intended. Our original contributions include the custom body CAD design, C-based movement library adaptation, environmental mapping algorithm, WiFi control interface, gyro drift mitigation, autonomous sweep state machine, and visualization scripts.</p>

    <p>While quadruped locomotion and sensor fusion are extensively patented by companies like Boston Dynamics, our educational implementation uses different mechanisms (hobby servos), operates at smaller scale, and employs publicly known techniques. As an educational project for ECE 4760 at Cornell University, we properly cite all external resources, and the integration work and software architecture represent our original engineering effort.</p>
</section>

        </main>
    </div>
</body>
</html>
